<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="Render and Diffuse">
    <meta name="keywords" content="Imitation Learning, Diffusion Policy, Robot Manipulation">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Render and Diffuse: Aligning Image and Action Spaces for Diffusion-based Behaviour Cloning</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Render and Diffuse: Aligning Image and Action Spaces for
                        Diffusion-based Behaviour Cloning</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://x.com/vitalisvos19">Vitalis Vosylius<sup>1,2,*</sup></a>,</span>
                        </span>
                        <span class="author-block">
              <a href="https://younggyo.me/">Younggyo Seo<sup>1</sup></a>,</span>
                        </span>
                        <span class="author-block">
              <a href="https://github.com/JafarAbdi">Jafar Uru√ß<sup>1</sup></a>,</span>
                        </span>
                        <span class="author-block">
              <a href="https://stepjam.github.io/">Stephen James<sup>1</sup></a></span>
                        </span>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Dyson Robot Learning Lab, <sup>2</sup>Imperial College London</span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>*</sup>Work done during an internship at the Dyson Robot Learning Lab.</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                <a href="https://arxiv.org/pdf/2405.18196"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
</section>


<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <video id="teaser" autoplay muted loop playsinline height="100%">
                <source src="./static/videos/promo.mp4"
                        type="video/mp4">
            </video>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        In the field of Robot Learning, the complex mapping between high-dimensional observations such
                        as RGB images and low-level robotic actions, two inherently very different spaces, constitutes a
                        complex learning problem, especially with limited amounts of data. In this work, we introduce
                        Render and Diffuse (R&D) a method that unifies low-level robot actions and RGB observations
                        within the image space using virtual renders of the 3D model of the robot. Using this joint
                        observation-action representation it computes low-level robot actions using a learnt diffusion
                        process that iteratively updates the virtual renders of the robot. This space unification
                        simplifies the learning problem and introduces inductive biases that are crucial for sample
                        efficiency and spatial generalisation. We thoroughly evaluate several variants of R&D in
                        simulation and showcase their applicability on six everyday tasks in the real world. Our results
                        show that R&D exhibits strong spatial generalisation capabilities and is more sample efficient
                        than more common image-to-action methods.
                    </p>
                </div>
            </div>
        </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">

        <div class="rows">
            <!-- Animation. -->
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3">Render and Diffuse</h2>

                    <div class="content has-text-justified">
                        <!-- <br> -->
                    </div>
                    <img src="static/images/overview.png" class="interpolation-image"
                         alt="Interpolate start reference image."/>
                    </br>
                    </br>
                    <p>
                        Overview of Render and Diffuse (R&D). We introduce a strategy that unifies image and
                        action domains within a single image space (I), simplifying the learning process by reducing
                        the complexity of navigating between these different spaces. This approach uses a rendering
                        procedure and a known model of the robot to visually 'imagine' the spatial implications of the
                        robot's actions. Through a learned denoising process, these rendered actions are updated until
                        they closely align with the actions observed during the demonstrations. The architecture employs
                        a Transformer model, where different positional embeddings are added to the tokenized input.
                        These
                        token embeddings are processed with several self-attention layers and then decoded into
                        per-camera denoising directions in the image space, gripper actions, and noise added to the
                        ground truth actions in the action space.
                    </p>
                    </br>

                </div>
            </div>
        </div>
</section>


<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3">Robot Videos</h2>
                    <div class="content has-text-justified">
                        <!-- <br> -->
                    </div>
                </div>
            </div>
        </div>
</section>

<section class="hero is-light is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item item-toilet_3">
                    <video poster="" id="toilet_3" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/Individual_Tasks/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-box_3">
                    <video poster="" id="box_3" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/Individual_Tasks/2.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-sweep_3">
                    <video poster="" id="sweep_3" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/Individual_Tasks/3.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-apple_3">
                    <video poster="" id="apple_3" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/Individual_Tasks/4.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-open_3">
                    <video poster="" id="open_3" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/Individual_Tasks/5.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-close_3">
                    <video poster="" id="close_3" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/Individual_Tasks/6.mp4"
                                type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        <div class="content has-text-justified">
            <p class="subtitle has-text-centered">
                Videos are captured from an external camera showing the robot performing everyday tasks at 2x speed.
            </p>

        </div>
    </div>
</section>

<section class="hero is-light is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item item-toilet">
                    <video poster="" id="toilet" autoplay controls muted loop playsinline height="200%">
                        <source src="./static/videos/Individual_Tasks/toilet_seat_down.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-box">
                    <video poster="" id="box" autoplay controls muted loop playsinline height="200%">
                        <source src="./static/videos/Individual_Tasks/open_box.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-apple">
                    <video poster="" id="apple" autoplay controls muted loop playsinline height="200%">
                        <source src="./static/videos/Individual_Tasks/place_apple.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-sweep">
                    <video poster="" id="sweep" autoplay controls muted loop playsinline height="200%">
                        <source src="./static/videos/Individual_Tasks/sweep_cupboard.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-open">
                    <video poster="" id="open" autoplay controls muted loop playsinline height="200%">
                        <source src="./static/videos/Individual_Tasks/open_drawer.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-close">
                    <video poster="" id="close" autoplay controls muted loop playsinline height="200%">
                        <source src="./static/videos/Individual_Tasks/close_drawer.mp4"
                                type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        <div class="content has-text-justified">
            <p class="subtitle has-text-centered">
                Videos are composed of the actual images that the robot sees during the execution of the
                tasks and our devised rendered action representation.
                <br>
                Observations from the shoulder and wrist cameras are shown on the left and right sides of each video,
                respectively.
                <br>
                Different colours of the rendered grippers represent different action predictions in the future.
            </p>

        </div>
    </div>
</section>


<!--<section class="section" id="BibTeX">-->
<!--    <div class="container is-max-widescreen">-->
<!--        <h2 class="title">BibTeX</h2>-->
<!--        <pre><code>@article{,-->
<!--  author    = {},-->
<!--  title     = {},-->
<!--  journal   = {},-->
<!--  year      = {2024},-->
<!--}</code></pre>-->
<!--    </div>-->
<!--</section>-->

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        Template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
